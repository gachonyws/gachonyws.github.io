---
title: "하둡(Hadoop) 세부 (1/2)"
date: 2020-04-14
header:
  teaser: /assets/images/hadoop.png
  og_image: /assets/images/hadoop.png
categories:
  - hadoop
published: True
---

1. 하둡(hadoop)
   * 여러가지 모드(단일,의사,완전분산)
   * 기능의 이해(HDFS,MapReduce)
   * 아키텍쳐
   * 에코시스템

---

### 여러가지 모드

하둡은 세 가지 다른 모드로 사용된다.

- 단일모드(The standalone mode)

이 모드에서는 어떤 하둡 데몬도 시작할 필요가 없다. 대신에 하둡 작업을 단일 자바 프로세스로 실행하는 ~/Hadoop디렉토리/bin/hadoop만 호출하면 된다. 이 모드는 테스트 용도로만 추천하며 기본 모드이기 때문에 어떠한 다른 설정도 필요 없다. NameNode, DataNode, JobTracker, TaskTracker 같은 모든 데몬은 단일 자바 프로세스로 작동한다.

- 의사모드(The pseudo mode)

모든 노드를 위해 하둡을 설정해줘야 하는 모드이다. 각 하둡 컴포넌트나 데몬을 위해 별도의 자바 가상 머신(JVM)이 생성되어 단일 호스트상의 작은 클러스터처럼 구성된다.

- 완전 분산 모드(The full distributed mode)

여러 컴퓨터상에 분산되는 모드이다. 전용으로 할당된 호스트가 하둡 컴포넌트를 위해 설정된다. 그러므로 개별 JVM프로세스가 모든 데몬을 위해 실행된다.


---

### 기능의 이해

- 하둡 분산 파일 시스템(HDFS: Hadoop Distributed File System)
- 맵리듀스(MapReduce)

하둡은 위의 두 가지 핵심 개념을 위해 특별히 디자인되었다. 둘 모두 분산 작업에 관련되었다. **HDFS의 핵심은 분산 데이터 저장, 맵리듀스는 분산 데이터상에서 병렬처리를 수행하는 하둡의 핵심 부분!**

1. HDFS

하둡의 자체적인 랙 인식(rack-aware) 파일시스템이며, 하둡의 유닉스 기반 데이터 **저장 레이어다.** HDFS는 구글 파일시스템 개념에서 유래했다. 하둡의 중요한 특성은 데이터와 연산이 수많은 노드로 분할된다는 점이다. 그리고 애플리케이션 연산은 데이터에 가까운 곳에서 병렬로 실행된다. HDFS에서 데이터 파일은 클러스터에서 연속된 블록으로 복제된다. 하둡 클러스터는 단순히 범용 서버를 추가해 연산 용량, 저장 용량. I/O 대역폭을 확장한다. 애플리케이션에서 HDFS에 접근하는 방법은 많다. 기본적으로 HDFS는 애플리케이션에서 사용할 자바API를 제공한다.

야후는 가장 큰 하둡 클러스터가 4,000대 서버였던 것을 40,000대의 서버로 확장해 40PB의 애플리케이션 데이터를 저장하고 있다. 또한 백여 개 이상의 세계적인 기업들이 하둡을 사용중이다.

- HDFS의 특성
  - 내고장성(Fault toletant): 시스템의 일부가 고장이 나도 전체에는 영향을 주지 않고, 항상 시스템의 정상 작동을 유지하는 능력.
  - 범용 하드웨어에서 작동
  - 큰 데이터 처리 가능
  - 마스터 슬레이브 패러다임
  - 한 번의 파일 접근 쓰기만 가능

2. MapReduce

맵리듀스는 큰 규모의 클러스터에 분산된 대용량 데이터셋을 **처리** 하기 위한 프로그래밍 모델이다. 맵리듀스의 프로그래밍 패러다임은 하둡 클러스터로 구성된 수천 대의 서버에서 대규모의 데이터 처리를 수행할 수 있게 해준다. 하둡 맵리듀스는 구글 맵리듀스에서 유래했다.

애플리케이션을 쉽게 작성하기 위한 소프트웨어 프레임워크이며, 범용 하드웨어로 이루어진 큰 규모의 클러스터(수천 대의 노드)에서 대량의 데이터(수 테레바이트의 데이터셋)를 신뢰성과 내고장성을 보장하며 **병렬로 처리** 할 수 있게 한다. 이러한 맵리듀스 패러다임은 맵과 리듀스 단계로 나뉘며 주로 데이터의 키와 밸류의 쌍을 다루는 역할을 한다. 맵과 리듀스 태스크는 클러스터에서 순서대로 실행되며, 맵 단계의 출력이 리듀스 과정의 입력이 된다.

- 맵 과정: 일단 데이터가 분할되고 나서, 데이터셋은 맵 태스크를 수행하기 위해 태스크 트래커에 할당된다. 맵 단계의 출력으로 키와 값 쌍을 쓰면서(emit) 데이터에 대한 기능적인 작업이 수행될 것이다.
- 리듀스 과정: 그 후에 마스터 노드는 모든 하부 문제에 대한 해답을 수집하고 출력을 생성하기 위해 그 해답을 결합한다. 이 출력값이 처음 해결하려고 했던 문제에 대한 해답이다.

병렬로 연산 작업을 수행하는 일반적인 다섯 단계는 다음과 같다

1. Map()의 입력을 준비: 입력 데이터가 행 단위로 들어와 각 행에 대해 키와 값 쌍을 써준다. 혹은 요구사항에 따라 명시적으로 변경할 수도 있다.
  - 맵 입력: list(k1,v1)

2. 사용자가 제공한 Map() 코드를 수행
  - 맵 출력: list(k2,v2)

3. 리듀스 프로세스를 위해 Map 출력을섞는다(shuffle). 동일한 키로 섞어서 그룹으로 묶고 나서 같은 리듀서에 입력한다.

4. 사용자가 제공한 Reduce() 코드를 수행: 이 과정에서는 개발자가 구현한 리듀서 코드가 데이터를 키와 값으로 처리하고 값을 써준다.

5. 최종 출력을 생성: 마지막으로 마스터 노드에서 모든 리듀서 출력을 수집하고 텍스트 파일로 쓴다.

---

### 아키텍쳐

1. HDFS 아키텍쳐

HDFS는 마스터/슬레이브 아키텍쳐로 표현할 수 있다. HDFS 마스터를 네임 노드(NameNode)라고 하고 슬레이브를 데이터 노드(DataNode)라고 한다.

네임 노드(NameNode)는 파일시스템 이름공간을 관리하고 클라이언트의 파일 접근(열기,닫기,이름 바꾸기 등)을 조정한다. 입력 데이터는 불록들로 나뉘며 어느 블록이 어떤 데이터 노드에 저장될 것인지를 알려준다.

데이터 노드(DataNode)는 분할된 데이터셋 복제본을 저장하고 요청에 대해 데이터를 제공해주는 슬레이브 컴퓨터다. 또한 블록 생성과 삭제도 수행한다.

HDFS의 내부적인 메커니즘에 의해서 파일은 하나 이상의 블록으로 나뉘고, 이 블록들은 데이터 노드 그룹에 저장된다. 복제 계수를 3으로 하는 일반적인 환경에서, HDFS 정책은 첫 번째 복사본은 해당 노드에, 두 번째 복사본은 같은 랙의 다른 노드에, 세번째 복사본은 다른 랙에 있는 다른 노드에 저장한다. HDFS가 큰 파일을 지원하도록 디자인되었기 때문에 HDFS 블록 크기는 64MB로 정의된다. 필요에 따라 이 값을 증가시킬 수 있다.

- HDFS 컴포넌트
HDFS는 다음 컴포넌트를 포함하는 마스터/슬레이브 아키텍처로 관리된다.
  - 네임 노드: HDFS 시스템의 마스터다. 디렉토리, 파일을 유지하고 데이터 노드에 존재하는 블록을 관리한다.
  - 데이터 노드: 각 컴퓨터에 배포되어 실제 스토리지를 제공하는 슬레이브다. 클라이언트로부터 읽기, 쓰기 요청을 처리할 책임이 있다.
  - 보조 네임 노드: 주기적으로 체크포인트를 수행할 책임이 있다. 그래서 언제라도 네임 노드가 고장나면, 체크포인트에 저장된 스냅샷 이미지로 대체될 수 있다.
{: .notice--info}

2. 맵리듀스 아키텍쳐

맵리듀스 또한 마스터/슬레이브 아키텍처로 구현되었다. 전형적인 맵리듀스는 잡 제출(submission), 잡 초기화, 태스크 할당, 진행 상황 갱신, 잡 완료 연관 활동을 포함하여 주로 잡 트래커에 의해 관리되고 태스크 트래커에 의해 실행된다. 클라이언트 애플리케이션은 잡 트래커에 잡을 제출한다. 그 후 입력이 클러스터 상에 분산된다. 그런 다음 잡 트래커는 처리될 맵과 리듀서의 수를 계산한다. 잡 트래커는 태스크 트래커에게 잡 실행을 시작하도록 명령한다. 태스크 트래커는 리소스를 로컬 컴퓨터에 복사하고, 데이터에 대해 수행할 맙과 리듀스 프로그램을 위해 JVM을 시작한다. 그리고 태스크 트래커는 잡 트래커에게 주기적으로 갱신 메시지를 보내는데 이 메시지는 JobID, 잡 상태, 리소스 사용량을 갱신하는데 도움을 주는 하트비트(heartbeat)로 여길 수 있다.

- 맵 리듀스 컴포넌트
맵리듀스는 다음 컴포넌트를 포함하는 마스터/슬레이브 아키텍처로 관리된다.
  - 잡 트래커: 맵리듀스 시스템의 마스터노드이며 클러스터(태스크 트래커)에서 잡과 리소스를 관리한다. 잡 트래커는 각 맵을 처리될 실제 데이터에 가까운 태스크 트래커에 스케줄링하려고 시도해 해당 기반 블록에 대한 데이터 노드와 같은 곳에서 잡을 수행하게 된다.
  - 태스크 트래커: 각 컴퓨터에 배포되는 슬레이브다. 잡 트래커의 지시로 맵과 리듀스 태스크를 실행할 책임이 있다.
{: .notice--info}

![architecture_img](/assets/images/hadoop_architecture.png)

---

### 에코시스템

하둡은 위의 두 기능(저장,처리)만으로는 데이터를 수집하고 분석하기엔 부족하다. 따라서 하둡 을 보완하기 위한 하둡 기반의 소프트웨어들을 에코시스템이라고 한다.

![ecosystem_img](/assets/images/hadoop_ecosystem.png)


- 머하웃(Mahout): 데이터마이닝 라이브러리. 가장 많이 사용되는 데이터 마이닝 알고리즘을 포함하며, 군집화, 분류, 회귀, 통계 모델링 수행을 위한 확장 가능한 머신러닝 알고리즘을 보유하고 있다.

- Hbase: 분산 빅데이터 저장소. 빅데이터에 대한 임의(random) 및 실시간(real-time) 읽기/쓰기 접근을 제공한다. 구글 빅테이블에서 영감을 받아 고안되어 컬럼 기반의 데이터 저장 모델로 디자인되엇다.

- Hive: 페이스북에서 개발한 하둡 기반의 데이터웨어하우징 프레임워크이며, HiveQL 같은 하둡 맵리듀스를 고도로 추상화한 SQL 유사 언어를 이용해 사용자가 쿼리를 할 수 있게 해준다. 맵리듀스 경험이 없는 SQL 프로그래머가 데이터웨어하우스를 사용할 수 있게 해주며 실시간 쿼리 처리를 위해 비지니스 인텔리전스나 시각화 툴에 대한 연동을 쉽게 만들어 준다.

- Pig: 자체적인 SQL 유사 언어인 Pig Latin을 통해 대규모 데이터셋을 분석하기 위한 하둡 기반의 오픈소스 플랫폼이다. 피그는 대량의 복잡한 데이터 병렬 연산을 위한 간단한 작업과 프로그래밍 인터페이스를 제공한다. 피그와 하이브는 맵리듀스를 쉽게 만든 데이터 분석 툴.

- 스쿱(Sqoop): 관계형 데이터베이스, 데이터웨어하우스, 비관계형 데이터스가 대량의 데이터를 새로운 방법으로 빠르게 전송하는 하둡 데이터 처리 플랫폼을 제공한다. 하둡 HDFS에서 데이터를 가져오고 RDBMS로 내보내는 쌍방향의 데이터 도구이다.

- 주키퍼(Zookeeper): 분산 코디네이터로 하둡, 하이브, 피그, HBase등의 여러 프로젝트를 관리하는 데 사용된다. 서버/클라이언트 두 부분으로 나뉘며, 설정 정보, 네이밍 서비스, 분산 동기화 및 그룹 서비스를 유지하기 위한 중앙 집중화된 서비스이다.

이외에도 다양한 툴들이 존재한다.
